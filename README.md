# ğŸ§  MiniHealthLM

A **lightweight and secure Transformer language model** trained from scratch on a single medical textbookâ€”[*Grant's Atlas of Anatomy*](https://uotechnology.edu.iq/dep/bme/english/Pages/Lectures/anatomy%20first%20course/Grant's%20Atlas%20of%20Anatomy.pdf).  
MiniHealthLM is designed for rapid experimentation, low-resource environments, and educational research in healthcare language modeling.

---

## ğŸ¯ Project Purpose

This project explores how to build and pretrain a **domain-specific LLM from scratch** using:

- Only a single PDF textbook as the corpus
- Custom tokenizer trained from that corpus
- Privacy-aware data sanitization
- Lightweight Qwen-style Transformer architecture

It aims to demonstrate how **LLMs can be scaled down** and securely trained for specific domains like **medicine and anatomy**, while remaining fully transparent and modular.

---

## ğŸ—ï¸ What I Built

MiniHealthLM includes a complete **end-to-end pretraining pipeline**:

- ğŸ“– PDF âœ plain text (`extract_text.py`)
- ğŸ§¼ Secure preprocessing (`sanitize_data.py` using PII filters)
- ğŸ”¡ Tokenizer training (Byte-Level BPE)
- ğŸ§  Transformer pretraining (Qwen-style GQA + RoPE + RMSNorm)
- ğŸ’¾ Checkpointing & loss tracking with TQDM

Training Dataset:  
ğŸ“˜ **Grant's Atlas of Anatomy**  
â¡ï¸ [PDF Link](https://uotechnology.edu.iq/dep/bme/english/Pages/Lectures/anatomy%20first%20course/Grant's%20Atlas%20of%20Anatomy.pdf)

---

## ğŸ§¬ Architecture Overview

| Component       | Description                          |
|----------------|--------------------------------------|
| Embedding       | Token Embedding (Byte BPE)          |
| Layers          | 20 Transformer blocks                |
| Hidden Size     | 3072                                 |
| Attention       | Grouped Query Attention (GQA)        |
| Heads           | 16 query heads, 4 key-value heads    |
| Norm            | RMSNorm                              |
| Positional Bias | Rotary Position Embeddings (RoPE)    |
| FFN             | SiLU MLP (4x expansion)              |
| Context Length  | 1024 tokens                          |
| Params (est.)   | ~0.6 Billion                         |

---

## âœ¨ Novelty

- âš•ï¸ **Domain-First**: Model trained purely on anatomical medical text
- ğŸ” **Secure Pretraining**: Removes PII and sensitive content before tokenization
- ğŸ”¬ **Small Yet Complete**: Implements modern LLM features like RoPE, GQA, and RMSNorm
- ğŸ§  **Trained From Scratch**: No pretraining dependency or transfer â€” 100% domain-rooted
- âš¡ **Single-GPU Ready**: Efficient enough to run on local or lab hardware

---

## ğŸš€ Quickstart

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Extract Text from PDF
```bash
python extract_text.py
```

### 3. Sanitize (Remove PII)
```bash
python sanitize_data.py
```

### 4. Train Tokenizer
```bash
python train_tokenizer.py
```

### 5. Launch Pretraining
```bash
python train.py
```

---

## ğŸ—‚ï¸ Folder Structure

```
MiniHealthLM/
â”œâ”€â”€ checkpoints/              # Saved model checkpoints
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus.pdf            # Grant's Atlas PDF
â”‚   â”œâ”€â”€ corpus.txt            # Cleaned training text
â”‚   â””â”€â”€ tokenizer/            # Tokenizer files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py             # Model settings
â”‚   â”œâ”€â”€ model.py              # Qwen-style transformer
â”‚   â”œâ”€â”€ dataset.py            # DataLoader
â”‚   â””â”€â”€ utils.py              # Checkpointing utilities
â”œâ”€â”€ extract_text.py
â”œâ”€â”€ sanitize_data.py
â”œâ”€â”€ train_tokenizer.py
â”œâ”€â”€ test_tokenizer.py
â”œâ”€â”€ train.py
â””â”€â”€ requirements.txt
```

---

## ğŸ‘¨â€ğŸ’» Author

**Elias Hossain**  
_Machine Learning Researcher | Secure LLMs | Biomedical NLP_

[![GitHub](https://img.shields.io/badge/GitHub-EliasHossain001-blue?logo=github)](https://github.com/EliasHossain001)
